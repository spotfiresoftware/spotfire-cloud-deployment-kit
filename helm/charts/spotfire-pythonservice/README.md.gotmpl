{{ template "chart.header" . }}

{{ template "chart.deprecationWarning" . }}

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

{{ template "chart.maintainersSection" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

## Overview

This chart deploys the [TIBCO SpotfireÂ® Service for Python](https://docs.tibco.com/pub/sf-pysrv/latest/doc/html/TIB_sf-pysrv_install/pyinstall/topics/the_tibco_spotfire_service_for_python.html) service (Python service) on a [Kubernetes](http://kubernetes.io/) cluster using the [Helm](https://helm.sh/) package manager.

The Python service pod includes:
- A [Fluent Bit](https://fluentbit.io/) sidecar container for log forwarding.
- Service annotations for [Prometheus](https://prometheus.io/) scrapers. The Prometheus server discovers the service endpoint using these specifications and scrapes metrics from the exporter.
- A predefined configuration for horizontal pod autoscaling with [KEDA](https://keda.sh/docs) and Prometheus.

This chart is tested to work with [Elasticsearch](https://www.elastic.co/elasticsearch/), [Prometheus](https://prometheus.io/) and [KEDA](https://keda.sh/).

## Prerequisites

- A deployed Spotfire Server release using the [Spotfire Server](../spotfire-server/README.md) chart.

## Usage

### Installing

1. Export the `SPOTFIRE_SERVER` value to connect to the `spotfire-server` service:
    ```bash
    export SPOTFIRE_SERVER=$(kubectl get services --selector=app.kubernetes.io/part-of=spotfire,app.kubernetes.io/name=spotfire-server --output=jsonpath={.items..metadata.name})
    ```
2.  Forward the logs to the `log-forwarder` service:
    ```bash
    export LOG_FORWARDER=$(kubectl get services --selector=app.kubernetes.io/part-of=spotfire,app.kubernetes.io/name=log-forwarder --output=jsonpath={.items..metadata.name})
    ```
3. Install this chart with the release name `my-release` and custom values from `my-values.yaml`:
    ```bash
    helm install my-release . \
        --set acceptEUA=true \
        --set global.spotfire.image.registry="127.0.0.1:32000" \
        --set global.spotfire.image.pullPolicy="Always" \
        --set nodemanagerConfig.serverBackendAddress="$SPOTFIRE_SERVER" \
        --set logging.logForwarderAddress="$LOG_FORWARDER" \
        -f my-values.yaml
    ```

**Note**: This TIBCO Spotfire Helm chart requires setting the parameter `acceptEUA` (or the parameter `global.spotfire.acceptEUA`) to the value `true`.
By doing so, you agree that your use of the TIBCO Spotfire software running in the managed containers will be governed by the terms of the [Cloud Software Group, Inc. End User Agreement](https://terms.tibco.com/#end-user-agreement).

**Note**: You must provide your private registry address where the Spotfire containers are stored.

See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation.

### How to add additional Python packages

Configure `volumes.packages` to mount a persistent volume, which can contain additional Python packages for the Python service to use. See the **Values** section for more information on mounting the volume for the packages.

#### How to install packages using pip to a target folder

You can populate a packages folder by following the instructions in [Installing Python Packages Manually](https://docs.tibco.com/pub/sf-pysrv/latest/doc/html/TIB_sf-pysrv_install/pyinstall/topics/installing_python_packages_manually.html). Here is an example of how to create a packages folder from a requirements.txt file:

```bash
python -m pip install --target=/local/path/to/packages -r requirements.txt
```

**Note:** For spotfire-pythonservice to use the packages, the packages must be copied to a PersistentVolume that you create and then provide to the helm chart during installation. 

#### Installing Python packages directly to a PersistentVolume using a Kubernetes Job 

The file [deploy-packages-to-pv-with-job.yaml](deploy-packages-to-pv-with-job.yaml) is an example of how to use a Kubernetes Job, PersistentVolumeClaim, and ConfigMap to create and populate a PersistentVolume containing Python packages.  

You might need to change some values in the file. For example, change the storageClassName for PersistentVolumeClaim to StorageClass from 'nfs-client' to one that exists in your environment. The full set of commands would look something like the following example.

```bash
# kubectl will create the PersistentVolumeClaim 'packages-pvc' pointing to PersistentVolume containing the installed Python packages.
kubectl apply . -f deploy-packages-to-pv-with-job.yaml

# When you install the spotfire-pythonservice Helm chart, pass in packages-pvc. 
helm install my-release --set volumes.packages=packages-pvc <... additional helm install arguments>
``` 
#### Configuration

To set [Custom configuration properties](https://docs.tibco.com/pub/sf-pysrv/latest/doc/html/TIB_sf-pysrv_install/_shared/install/topics/custom_configuration_properties.html), add the name of the property as a key under the `configuration` section in your helm values.

Example:
```
configuration:
  # The maximum number of Python engine sessions that are allowed to run concurrently in the Python service.
  engine.session.max: 5

  # The number of Python engines preallocated and available for new sessions in the Python service queue.
  engine.queue.size: 10
```

### Uninstalling

To uninstall/delete the `my-release` deployment:
```bash
helm uninstall my-release
```

See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation.

### Scaling

For scaling the `my-release` deployment, do a helm upgrade, providing the target number of pod instances in the `replicaCount` variable.
```bash
helm upgrade --install my-release . --reuse-values --set replicaCount=3
```

#### Autoscaling with KEDA

To use [KEDA](https://keda.sh/docs) for autoscaling, first install KEDA in the Kubernetes cluster. You must also install a Prometheus instance that scrapes metrics from the Spotfire pods.

Example: A `values.yaml` snippet configuration for enabling autoscaling with KEDA:
```
resources:
  limits:
    cpu: 5
kedaAutoscaling:
  enabled: true
  spotfireConfig:
    prometheusServerAddress: http://prometheus-server.monitor.svc.cluster.local
  threshold: 3
  minReplicas: 1
  maxReplicas: 3
```

The `spotfire-pythonservice` has the following autoscaling defaults:
- The default metric:  `spotfire_service_queue_engines_inUse`.
- The default query:   The sum of _service_queue_engines_inUse_ of the Python service instances.

The counter _serviceQueueEnginesInUse_ provides the total number of engines currently executing.
By default, the Python service has `number of cores - 1` available slots, which means that `kedaAutoscaling.threshold` should be synchronized with `resources.limits.cpu`.
Set the `kedaAutoscaling.threshold` to be lower than `resources.limits.cpu` so you can scale out before all the available capacity is taken.

**Note**:  Clients requesting a slot typically wait until a slot is available.

For more information, see [Monitoring Spotfire Service for Python using JMX](https://docs.tibco.com/pub/sf-pysrv/latest/doc/html/TIB_sf-pysrv_install/_shared/install/topics/monitoring_the_service_using_jmx.html).

**Note**: You can tune `nodemanagerConfig.preStopDrainingTimeoutSeconds` and other timeouts (for example, `engine.execution.timeout` and `engine.session.maxtime`) so that long-running jobs are not aborted prematurely when an instance is stopped to scale in.
See [Engine Timeout](https://docs.tibco.com/pub/sf-pysrv/latest/doc/html/TIB_sf-pysrv_install/_shared/install/topics/engine_timeout.html) for more details.

For more advanced scenarios, see [kedaAutoscaling.advanced](https://keda.sh/docs/latest/concepts/scaling-deployments/#advanced) and [kedaAutoscaling.fallback](https://keda.sh/docs/latest/concepts/scaling-deployments/#fallback).

Additionally, you can define your own [custom scaling triggers](https://keda.sh/docs/latest/concepts/scaling-deployments/#triggers). Helm template functionality is available:
```
kedaAutoscaling:
  triggers:
  # {list of triggers to activate scaling of the target resource}
```

### Upgrading

See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/) for command documentation.

#### Upgrading helm chart version

**Note**: When you upgrade to a newer Spotfire Server version and newer Spotfire services versions, upgrade the Spotfire Server first, and then upgrade the Spotfire services.

The following parameters in values.yaml have been changed, moved or renamed and must be taken into consideration when upgrading the release.

##### Version 0.1.5

| New key | Old key | Comment |
| ------- | ------- | ------- |
| `acceptEUA` | | Accept the End User Agreement by setting `acceptEUA` or `global.spotfire.acceptEUA` to **true** or else Helm release will not install. |
| `global.spotfire.acceptEUA` | | Same as `acceptEUA` but as a global value. |
| `configuration` | `config.conf/custom.properties` | Exposes the service configuration 'custom.properties' as Helm values. |
| | `config.log4j2.xml` | Removed. Log level can be set with `logging.logLevel`. |
| | `volumes.packages.mountPath` | Removed. `mountPath` is now hardcoded to **/opt/packages**. |

{{ template "chart.valuesSection" . }}
