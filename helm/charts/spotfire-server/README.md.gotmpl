{{ template "chart.header" . }}

{{ template "chart.deprecationWarning" . }}

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

{{ template "chart.maintainersSection" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

## Overview

This chart deploys the [TIBCO Spotfire® Server](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/introduction_to_the_tibco_spotfire_environment.html) on a [Kubernetes](http://kubernetes.io/) cluster using the [Helm](https://helm.sh/) package manager.

Using this chart, you can also deploy the following:
- The required Spotfire Server database schemas on a supported database server (for example, Postgres).
- A reverse proxy ([HAProxy](https://www.haproxy.org/)) for accessing the Spotfire Server cluster service, with session affinity for external HTTP access.
- An ([Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)) with routing rules for accessing the configured reverse proxy.
- Shared storage locations ([Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)) for the Spotfire library import and export, custom jars, deployment packages, and other purposes.

The Spotfire Server pod includes:
- A [Fluent Bit](https://fluentbit.io/) sidecar container for log forwarding.
- Service annotations for [Prometheus](https://prometheus.io/) scrapers. The Prometheus server discovers the service endpoint using these specifications and scrapes metrics from the exporter.
- Predefined configuration for horizontal pod autoscaling with [KEDA](https://keda.sh/docs) and Prometheus.

This chart is tested to work with [NGINX Ingress Controller](https://github.com/kubernetes/ingress-nginx), [Elasticsearch](https://www.elastic.co/elasticsearch/), [Prometheus](https://prometheus.io/) and [KEDA](https://keda.sh/).

## Prerequisites

- Familiarity with Kubernetes, containers and Helm chart concepts and usage
- Helm 3+
- Ingress controller (optional)
- PV (Persistent Volume) provisioner support in the underlying infrastructure (optional)
- A supported database server for use as the Spotfire database. For information on supported databases, see [Spotfire Server requirements](https://docs.tibco.com/pub/spotfire/general/sr/sr/topics/system_requirements_for_spotfire_products.html).

**Note**: You can install the database server in the same Kubernetes cluster or on premises. Alternatively, you can use a cloud database service.

## Usage

### Installing

To install the chart with the release name `my-release` and the values from the file `my-values`:
```bash
helm install my-release -f my-values.yml .
```

**Note**: The Spotfire Server chart requires some variables to start (such as database server connection details). 
See examples and variables description below.

**Note**: You can use any of the Spotfire supported databases with these recipes, and you can choose to run the database on containers, VMs, bare metal servers, or use a cloud database service.

For more information on how to configure the different supported databases using this Helm chart, 
see the [Supported databases configuration table](#supported-databases-configuration) and the [Values table](#values).

See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation.

#### Example: Installing with an existing database

This example shows how to deploy a Spotfire Server using an already existing supported database.

Deploy the Spotfire Server using an existing database:
```bash
helm install my-release . \
    --set acceptEUA=true \
    --set database.bootstrap.databaseUrl="$DB_BOOTSTRAP_URL" \
    --set database.create-db.databaseUrl="$DB_URL" \
    --set database.create-db.adminUsername="$DB_ADMIN" \
    --set database.create-db.adminPassword="$DB_PASSWORD" \
    --set database.create-db.enabled=true \
    -f my-values.yml
```

**Note**: This TIBCO Spotfire Helm chart requires setting the parameter `acceptEUA` or the parameter `global.spotfire.acceptEUA` to the value `true`.
By doing so, you agree that your use of the TIBCO Spotfire software running in the managed containers will be governed by the terms of the [Cloud Software Group, Inc. End User Agreement](https://terms.tibco.com/#end-user-agreement).

**Note**: Before running the previous command, set the proposed variables in your environment, or edit the command, replacing the proposed variables with the corresponding values.

For more information, see:
- [create-db](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/create-db.html) command documentation.
- [bootstrap](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/bootstrap.html) command documentation.
- and the [Values table](#values) section

Additional database connection configuration is typically done through the JDBC connection properties in the connection url and varies between different database and driver vendors. 
For example, for PostgreSQL, see [Postgres JDBC](https://jdbc.postgresql.org/documentation/head/connect.html).

In some specific cases there is also a need to place additional files in the container and supply the absolute path of these files in the connection url. 
See [Volumes](#volumes) section for details.

#### Example: Installing with a new database

This example shows how to deploy a Spotfire Server using a PostgreSQL database in a Kubernetes cluster using Helm charts. 
The database is deployed as a container and, by default, the PostgreSQL Helm chart creates its own PVC for persistent data storage. 

Steps:

1. Add the Bitnami charts repository and install a Postgresql database using the [Bitnami's PostgreSQL chart](https://artifacthub.io/packages/helm/bitnami/postgresql):
    ```bash
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm install vanilla-tssdb bitnami/postgresql
    ```

2. Export the Postgresql autogenerated random password:
    ```bash
    export POSTGRES_PASSWORD=$(kubectl get secret --namespace default vanilla-tssdb-postgresql -o jsonpath="{.data.postgres-password}" | base64 --decode)
    ```

3. Install the Spotfire Server chart using the release name `vanilla-tss`:
    ```bash
    helm upgrade --install vanilla-tss . \
        --set acceptEUA=true \
        --set global.spotfire.image.registry="127.0.0.1:32000" \
        --set global.spotfire.image.pullPolicy="Always" \
        --set database.bootstrap.databaseUrl="jdbc:postgresql://vanilla-tssdb-postgresql.default.svc.cluster.local/" \
        --set database.create-db.databaseUrl="jdbc:postgresql://vanilla-tssdb-postgresql.default.svc.cluster.local/" \
        --set database.create-db.adminUsername=postgres \
        --set database.create-db.adminPassword="$POSTGRES_PASSWORD" \
        --set database.create-db.enabled=true \
        --set configuration.site.publicAddress=http://localhost/
    ```

   **Note**: You must provide your private registry address where the Spotfire containers are stored.

   **Note**: This example assumes that your Spotfire container images are located in a configured registry at 127.0.0.1:32000. 
   See the Kubernetes documentation for how to [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/), 
   and how to configure a local registry in your Kubernetes distribution (for example, [microk8s built-in registry](https://microk8s.io/docs/registry-built-in)).

4. Export the autogenerated Spotfire admin password:
    ```bash
    export SPOTFIREADMIN_PASSWORD=$(kubectl get secrets vanilla-tss-spotfire-server -o jsonpath="{.data.SPOTFIREADMIN_PASSWORD}" | base64 --decode)
    ```

5. Export the autogenerated Spotfire database password:
    ```bash
    export SPOTFIREDB_PASSWORD=$(kubectl get secrets vanilla-tss-spotfire-server -o jsonpath="{.data.SPOTFIREDB_PASSWORD}" | base64 --decode)
    ```

6. After some minutes, you can access the Spotfire Server web interface in `configuration.site.publicAddress`.

For more information on Spotfire, see the [TIBCO Spotfire® Server - Installation and Administration](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/administration.html) documentation.

For more information on PostgreSQL deployment using the chart in the example, see the [Bitnami's PostgreSQL chart](https://artifacthub.io/packages/helm/bitnami/postgresql) documentation.

### Uninstalling

To uninstall the `my-release` deployment:
```bash
helm uninstall my-release
```

See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation.

#### Deleting any remaining resources

Normally, all services and pods are deleted using `helm uninstall`, but occasionally you might need to manually delete existing Spotfire persistent volume claims or not completed jobs due to interrupted operation or incorrect setup.

To delete unused persistent volume claims, you can first list the persistent volume claims:
```bash
kubectl get pvc
```

And then, delete only the persistent volume claims that you do not want to keep. For example:
```bash
kubectl delete pvc data-my-release-postgresql-0
```

To delete old release jobs, you can first list the jobs:
```bash
kubectl get jobs
```

And then, delete the ones that you do not want to keep. For example:
```bash
kubectl delete job.batch/my-release-spotfire-server-basic-config-job
```

Alternatively, delete the Kubernetes namespace that contains these resources.

### Scaling

For scaling the `my-release` deployment, you can do a helm upgrade, providing the target number of pod instances in the `replicaCount` variable.
```bash
helm upgrade --install my-release . --reuse-values --set replicaCount=3
```

#### Autoscaling with KEDA

To use [KEDA](https://keda.sh/docs) for autoscaling, first install it in the Kubernetes cluster. You must also install a Prometheus instance that scrapes metrics from the Spotfire pods.

Example: A `values.yml` snippet configuration for enabling autoscaling with KEDA:
```
kedaAutoscaling:
  enabled: true
  spotfireConfig:
    prometheusServerAddress: http://prometheus-server.monitor.svc.cluster.local
  threshold: 60
  minReplicas: 1
  maxReplicas: 3
```

The value specified for threshold determines the value that the query must reach before the service is scaled.

The `spotfire-server` has the following defaults:
- The default autoscaling metric is the `spotfire_OS_OperatingSystem_ProcessCpuLoad`.
- The default query is the sum of CPU usage (in percent) of all of the Spotfire Server instances.

With these default settings, if the total CPU usage for all instances is greater than the threshold, then another instance is started to scale out the service. 
If the total CPU usage for all instances falls under the threshold, then the service scales in.

For each multiple of the `kedaAutoscaling.threshold`, another instance is scaled out.
- With 1 replica, if the query value is at >60% CPU, another replica is created.
- With 2 replicas, if the query value is at >120%, another replica is created, and so on.

For more information, see the [HPA algorithm details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details).

**Note**: To allow time for draining sessions when scaling in, tune the values for `draining.minimumSeconds` and `draining.timeoutSeconds`.

For more advanced scenarios, see [kedaAutoscaling.advanced](https://keda.sh/docs/latest/concepts/scaling-deployments/#advanced) and [kedaAutoscaling.fallback](https://keda.sh/docs/latest/concepts/scaling-deployments/#fallback).

Additionally, you can define your own [custom scaling triggers](https://keda.sh/docs/latest/concepts/scaling-deployments/#triggers). Helm template functionality is available:
```
kedaAutoscaling:
  triggers:
  # {list of triggers to activate scaling of the target resource}
```

### Upgrading

#### Upgrading helm chart version

The following parameters in values.yaml have been changed, moved or renamed and must be taken into consideration when upgrading the release.

##### Version 0.1.5

| New key | Old key | Comment |
| ------- | ------- | ------- |
| `acceptEUA` | | Accept the End User Agreement by setting `acceptEUA` or `global.spotfire.acceptEUA` to **true** or else Helm release will not install. |
| `global.spotfire.acceptEUA` | | Same as `acceptEUA` but as a global value. |
| `haproxy.spotfireConfig.captures.forwardedForLength` | | |

##### Version 0.1.4

| New key | Old key | Comment |
| - | - | - |
| `configuration.draining.*` | `draining.*` | Moved |
| `configuration.encryptionPassword` | `encryptionPassword` | Moved |
| `configuration.preferExistingConfig` | `configuration.useExisting` | Moved |
| `configuration.site.*` | `site.*` | Moved |
| `configuration.spotfireAdmin.*` | `spotfireAdmin.*` | Moved |
| `configuration.spotfireAdmin.passwordExistingSecret.{key,name}` | `spotfireAdmin.existingSecret` | Changed. Only the password and not the username is read from the existingSecret |
| `database.bootstrap.passwordExistingSecret.{name,key}` | `database.bootstrap.existingSecret` | Changed. Only the password and not the username is read from the existingSecret |
| `configuration.applyKubernetesConfiguration` | Removed |

##### Version 0.1.3

| New key | Old key |
| - | - |
| _spotfireAdmin.username_ | _spotfireAdminUsername_ |
| _spotfireAdmin.password_ | _spotfireAdminPassword_ |

#### Upgrading the Spotfire Server version

When upgrading a Spotfire Helm chart, there are several considerations that you should keep in mind to ensure a smooth upgrade process.

Firstly it's important to understand whether the new Helm chart version comes with a new Spotfire Server version. If it does, you'll need to carefully consider the implications of upgrading to a new server version, and make sure that you understand any potential compatibility issues or changes in functionality. 

##### Checking the manual

It's a good idea to refer to the [Upgrading Spotfire](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/upgrading_spotfire.html) page in the "TIBCO Spotfire® Server and Environment - Installation and Administration" manual for any specific considerations or instructions related to the version you are upgrading to. This will ensure that you're aware of any known issues or steps that you need to follow to upgrade successfully.

##### Upgrading Spotfire Server and Spotfire services

If you're upgrading to a newer Spotfire Server version and Spotfire services versions, it's recommended to upgrade the Spotfire Server first and then upgrade the Spotfire services.

##### Disabling automatic database schema upgrade

If you prefer to disable the automatic Spotfire database schema upgrade, you can set the `database.upgrade` value to *false*. However, in this case, you must manually upgrade the database.

##### Backing up the database

To roll back an upgrade, it's necessary to back up the Spotfire database before upgrading to a newer Spotfire Server version. This will ensure that you have a copy of the database in its previous state, and that you can revert to this state if necessary. If you use external library storage, you should also create a snapshot of the external library storage that corresponds to the database backup state.

##### Verifying the upgrade

The Kubernetes job `config-job` is responsible for upgrading the Spotfire database. To verify that the upgrade was successful, you should check the `config-job` logs. This will help you to identify any issues or errors that may have occurred during the upgrade process, and to ensure that your Spotfire server database has been successfully upgraded.

The Kubernetes job `config-job` uses the Spotfire Server Upgrade Tool. For details, see [Run the Spotfire Server Upgrade Tool](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/run_the_spotfire_server_upgrade_tool.html).

## Configuration

#### Running custom spotfire-config tool tasks during helm install / upgrade

During a helm release upgrade or install, you can run a custom `spotfire-config` tool tasks to add a custom configuration or to run custom tasks.
There are three types of `config.sh` scripts that you can run:

- `configuration.configurationScripts` - Scripts that modify the Spotfire server configuration (`configuration.xml`).
- `configuration.commandScripts` - Scripts that do not modify the Spotfire server configuration (for example, for creating users, assigning licenses, and so on).
- `configuration.preConfigCommandScripts` - Same as commandScripts but these command will be run before the configuration is imported.

See the [Values](#values) section below for more details.

**Note**: To deploy SDN files during helm upgrade or install, see [Using persistent volumes for deploying SDNs/SPKs](#volume-for-deploying-sdnsspks).

Example: A `values.yml` snippet configuration for running custom `spotfire-config` tool tasks:
```yaml
configuration:
  configurationScripts:
    - name: my_custom_script
      script: |
        echo "This is an example custom configuration tasks. "
        set-config-prop --name=lifecycle.changes-monitoring.draining.timeout-seconds --value=180 --configuration="${CONFIGURATION_FILE}" --bootstrap-config="${BOOTSTRAP_FILE}"
    - name: my_second_script
      script: |
        echo "This script will be executed after the one above."
        echo "Scripts are executed in the order in which they appear the values file."

  commandScripts:
    - name: my_commands_script
      script: create-user --bootstrap-config="${BOOTSTRAP_FILE}" --tool-password="${TOOL_PASSWORD}" --username="my_new_user" --password="password" --ignore-existing=true
```

You can use environment variables in the scripts. These include, but are not limited to, `CONFIGURATION_FILE`, `BOOTSTRAP_FILE` and `TOOL_PASSWORD`.

For more information about `config.sh` scripts, see the Spotfire Server documentation about [scripting a configuration](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/scripting_a_configuration.html).

If your scripts require additional environment variables, to add environment variables from existing secrets or configmaps, use the `extraEnvVarsSecret` or `extraEnvVarsCM` chart variables.

#### Managing configuration on helm install / upgrade

The key `configuration.apply` controls when to apply the values under the `configuration` key level.
See the following table for a summary of the possible values, descriptions, and when to use each of them.

| Value | Description | When it is useful |
|-------|-------------|-------------------|
| always | Apply on every `helm install` or `helm upgrade`| When you prefer to manage the configuration always using `configuration` keys |
| initialsetup | Apply only on new Spotfire server installationn, if there is no configuration in the database | When you want to use `configuration` keys for the initial setup of the system, but you prefer to manage the configuration using an external tool |
| never | Do not apply | When you prefer to manage the configuration externally, without using `configuration` keys |

**Note**: When set to `always`, the configuration made from tools other than helm might be overwritten when doing a helm upgrade.

**Note**: It is required that Spotfire database contains a configuration that is compatible this this helm chart and Spotfire running in Kubernetes, see [config-job-scripts/default-kubernetes-config.txt](config-job-scripts/default-kubernetes-config.txt). You need to make sure a compatible configuration is active, either by manually setting a configuration or by using the value `always` or `initialsetup` (only during initial setup) in which case the configuration job will apply the configuration for you.

**Note**: If you prefer to manage the configuration externally, you can also set `configuration.preferExistingConfig` to true.
See the [Values](#values) section for more details.

## Additional / custom environment variables

You can use the following chart keys to add additional environment variables to the pods:
- `extraEnvVars`, `extraEnvVarsCM`, `extraEnvVarsSecret` - Extra environment variables for the `spotfire-server` pod
- `cliPod.extraEnvVars`, `cliPod.extraEnvVarsCM`, `cliPod.extraEnvVarsSecret` - Extra environment variables for the `cli` pod
- `configJob.extraEnvVars`, `configJob.extraEnvVarsCM`, `configJob.extraEnvVarsSecret` - Extra environment variables for the `config-job` pod

Use these keys to inject environment variables for usage in custom initialization and configuration scripts, or to set options for the Spotfire Server JVM and Tomcat.

Use `extraEnvVarsSecret` or `extraEnvVarsCM` to add environment variables from existing secrets or configMaps.

Example: A `values.yml` snippet configuration for JVM settings for the Spotfire Server:
```yaml
extraEnvVars:
  - name: CATALINA_INITIAL_HEAPSIZE
    value: 1024m
  - name: CATALINA_MAXIMUM_HEAPSIZE
    value: 2048m
  - name: CATALINA_OPTS
    value: "-Djava.net.preferIPv4Stack=true"
```

## Volumes

You can use volumes to mount external files into the containers file system to make them available to the containers. 
You can also use volumes to persist data that is written by the application to an external volume.

Setting up volumes permissions is usually handled by the Kubernetes administrators. See the Kubernetes documentation for best practices.

**Note**: You can create a pod running as the user `root` that uses a PersistentVolume or PersistentVolumeClaim to set the right permissions or to pre-populate the volume with jar files or library import files.

- For more information on volumes, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) and [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/).
- For more information using PersistentVolumes and PersistentVolumeClaims in a Pod, see [Configure a Pod to Use a PersistentVolume for Storage](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).

### Spotfire generic volumes

A generic way to use volumes with the `spotfire-server` chart is with the `extraVolumeMounts` and `extraVolumes` chart variables. 

There are specific variables for using these generic volumes on each pod: 
- For the `spotfire-server` pod, use `extraVolumeMounts` and `extraVolumes`.
- For the `cli` pod, use `cliPod.extraVolumeMounts` and `cliPod.extraVolumes`. 
- For the `config-job` pod, use `configJob.extraVolumeMounts` and `configJob.extraVolumes`.

Example:
```yaml
extraVolumeMounts:
  - name: example
    mountPath: /opt/tibco/example.txt
    subPath: example.txt
extraVolumes:
  - name: example
    persistentVolumeClaim:
      claimName: exampleClaim
```

### Spotfire specific volumes

You can use Kubernetes persistent volumes as a shared storage location for the _Spotfire library_ import and export location, custom jars, deployment packages, certificates, and other purposes.

You can use predefined chart values for the most common needed Spotfire volumes.
See the table below.

| Chart values                           | Purpose                                                                      |
|----------------------------------------|------------------------------------------------------------------------------|
| `volumes.certificates`                 | Use self-signed or custom certificates                                       |
| `volumes.customExt`                    | Use additional Java library files in the Spotfire Server                     |
| `volumes.customExtInformationservices` | Use additional Java library files for Information Services                   |
| `volumes.deployments`                  | Automatically deploy SDNs/SPKs files into a _Spotfire deployment area_       |
| `volumes.libraryImportExport`          | Common storage for Spotfire library import and export operations             |
| `volumes.troubleshooting`              | Persist `spotfire-server` JVM head dumps and information for troubleshooting |

**Note**: If a volume is not configured, it will not be used or the mountPath will use an emptyDir volume.

#### Volume for certificates

To use self-signed or custom certificates for connecting to LDAPS (such as `.jks` keystore files),
you can create a volume with the desired files and use
`volumes.certificates.existingClaim` to set the PersistentVolumeClaim to use.

*mountPath*:
- `/opt/tibco/tss/tomcat/certs` (spotfire-server pod)
- `/opt/tibco/spotfireconfigtool/certs` (config-job and cli pods)

**Note**: The `spotfire` user needs read permissions to the volume.

For more information on using self-signed certificates for LDAPS with the Spotfire Server, see [Configuring LDAPS](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/configuring_ldaps.html).

#### Volume for custom Java library files

To use custom jar files in the `spotfire-server` container,
you can create a PersistentVolume with the desired files use
`volumes.customExt.existingClaim` to set the PersistentVolumeClaim to use.

*mountPath*:
- `/opt/tibco/tss/tomcat/custom-ext` (spotfire-server pod)
- `/opt/tibco/spotfireconfigtool/custom-ext` (config-job and cli pods)

**Note**: The `spotfire` user needs read permissions for the volume.

For information on using additional Java library files for Spotfire Server, see:
- [Authentication towards a custom JAAS module
  ](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/authentication_towards_a_custom_jaas_module.html)
- [Post-authentication filter](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/post-authentication_filter.html)

#### Volume for additional JDBC drivers

To be able to access data from a JDBC-compliant data source with Information Services, use `volumes.customExtInformationservices`:

*mountPath*:
- `/opt/tibco/tss/tomcat/custom-ext-informationservices` (spotfire-server pod)
- `/opt/tibco/spotfireconfigtool/custom-ext-informationservices` (config-job and cli pods)

See [Installing database drivers for Information Designer](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/installing_database_drivers_for_information_designer.html) for more information.

#### Volume for deploying SDNs/SPKs

**Note**: By default, the deployment configuration job creates a deployment area using the release's `Spotfire.Dxp.sdn`, contained in the spotfire-deployment image. 
If this is sufficient for your use case, you can skip this section.
However, if you want to customize the Spotifre deployment areas structure and the packages to be deployed on each of them, you can follow instead the steps in this section.
For more information, see the keys under `configuration.deployment`.

To automatically deploy SDNs/SPKs files into a _Spotfire deployment area_, 
you can create a PersistentVolume with the desired files to mount on container start 
and use `volumes.deployments.existingClaim` to set the PersistentVolumeClaim to use.

Steps:
1. Copy the desired SDNs/SPKs in a folder (such as `Test/`) in the PersistentVolume. 
2. On helm install/upgrade, the `config-job` creates a _Spotfire deployment area_ with the folder name (if it does not exist), and the packages are deployed into that area.

Example: The following volume file structure creates the deployment areas "Production" and "Test", and deploys the provided SDN files in these respective deployment areas:
```txt
Production/Spotfire.Dxp.sdn
Test/Spotfire.Dxp.sdn
Test/Spotfire.Dxp.PythonServiceLinux.sdn
```

*mountPath*: `/opt/tibco/spotfireconfigtool/deployments` (config-job pod)

**Note**: The `spotfire` user needs read permissions for the volume.

**Note**: The Spotfire deployment area names are case-insensitive and have a maximum length of 25 characters. These are the valid characters:
* [a-z]
* [0-9]
* The underline character `_`
* The dash character `-`

For more information, see [Spotfire Deployments and deployment areas](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/deployments_and_deployment_areas.html).

#### Volume for library export and import

To use a common storage for Spotfire library import and export operations,
you can use `volumes.libraryImportExport.persistentVolumeClaim` or `volumes.libraryImportExport.existingClaim`
to control which PersistentVolume or PersistentVolumeClaim to use.

*mountPath*: `/opt/tibco/tss/tomcat/application-data/library` (spotfire-server pod)

**Note**: The `spotfire` user needs read and write permissions for the volume.

For more information, see [importing to library](https://docs.tibco.com/pub/sfire-analyst/latest/doc/html/en-US/TIB_sfire-analyst_UsersGuide/index.htm#t=lib%2Flib_importing_to_library.htm&rhsearch=export&rhsyns=%20)
and [exporting from library](https://docs.tibco.com/pub/sfire-analyst/latest/doc/html/en-US/TIB_sfire-analyst_UsersGuide/index.htm#t=lib%2Flib_exporting_from_library.htm&rhsearch=export&rhsyns=%20).

#### Volume for troubleshooting files

To persist `spotfire-server` JVM head dumps and information for troubleshooting when pods are removed, 
you can use `volumes.troubleshooting.persistentVolumeClaim` or `volumes.troubleshooting.existingClaim`
to control which PersistentVolume or PersistentVolumeClaim to use.

*mountPath*: `/opt/tibco/troubleshooting/jvm-heap-dumps` (spotfire-server pod)

**Note**: The `spotfire` user needs write permissions for the volume.

## Always-on Spotfire configuration CLI pod

When `cliPod.enabled` is set to `true`, an always-on Spotfire configuration CLI pod is deployed with the chart. You can use this pod to manage and configure the Spotfire environment.

Example: Get the bash prompt into the configuration CLI pod:
```bash
$ kubectl exec -it $(kubectl get pods -l "app.kubernetes.io/component=cli, app.kubernetes.io/part-of=spotfire" -o jsonpath="{.items[0].metadata.name}" ) -- bash
my-release-cli-859fdc8cdf-d58rf $
```

For more information, see [Configuration using the command line](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/configuration_using_the_command_line.html).

## Supported databases configuration

| Database              | driver class                                 | create-db.databaseUrl                                    | bootstrap.databaseUrl                                                | Additional parameters                    |
|-----------------------|----------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------|
| Postgres              | org.postgresql.Driver                        | jdbc:postgresql://databasehost:databaseport/             | jdbc:postgresql://databasehost:databaseport/                         |                                          | 
| Oracle                | oracle.jdbc.OracleDriver                     | jdbc:oracle:thin:@//databasehost:databaseport/service    | jdbc:oracle:thin:@//databasehost:databaseport/service                | oracleRootfolder, oracleTablespacePrefix |
| MSSQL                 | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport               | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databasename |                                          |
| AWS Postgres          | org.postgresql.Driver                        | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename             | doNotCreateUser = true                   | 
| Aurora Postgres       | org.postgresql.Driver                        | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename             | doNotCreateUser = true                   | 
| AWS Oracle            | oracle.jdbc.OracleDriver                     | jdbc:oracle:thin:@databasehost:databaseport/ORCL         | jdbc:oracle:thin:@databasehost:databaseport/ORCL                     | variant = rds                            |
| AWS MSSQL             | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport               | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databaseName | variant = rds                            |
| Azure Postgres        | org.postgresql.Driver                        | jdbc:postgresql://databasehost:databaseport/databasename | jdbc:postgresql://databasehost:databaseport/databasename             | doNotCreateUser = true                   | 
| Azure MSSQL           | com.microsoft.sqlserver.jdbc.SQLServerDriver | jdbc:sqlserver://databasehost:databaseport               | jdbc:sqlserver://databasehost:databaseport;DatabaseName=databaseName | variant = azure                          |
| Google Cloud Postgres | org.postgresql.Driver                        | jdbc:postgresql://databasehost:databaseport/             | jdbc:postgresql://databasehost:databaseport/databasename             | doNotCreateUser = true                   | 


{{ template "chart.valuesSection" . }}
