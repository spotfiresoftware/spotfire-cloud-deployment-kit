# Default values for webplayer.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## @param global.spotfire.image.registry Global Container image registry.
## @param global.spotfire.image.pullSecrets Global Container registry secret names as an array.
##
global:
  spotfire:
    image:
      # global.spotfire.image.registry -- The global container image registry. Used for tibco/spotfire container images unless it is overridden.
      registry:
      # global.spotfire.image.pullPolicy -- The global container image pull policy.
      pullPolicy: IfNotPresent
      # global.spotfire.image.pullSecrets -- The global container image pull secrets.
      ## E.g.
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
  serviceName: webplayer

nodemanagerConfig:
  # -- The spotfire-server service name. This value is evaluated as a helm template.
  serverBackendAddress: ""
  # -- The draining timeout after which the service is forcefully shut down.
  preStopDrainingTimeoutSeconds: 610

webplayerConfig:
  # -- The web player resource pool.
  resourcePool: ""

replicaCount: 1

image:
  # image.registry -- The image registry for spotfire-server, it overrides global.spotfire.image.registry value.
  registry:
  # image.repository -- The spotfire-server image repository.
  repository: tibco/spotfire-webplayer
  # image.tag -- The container image tag to use.
  tag: "12.1.1-1.1.0"
  # image.pullPolicy -- The spotfire-server image pull policy, It overrides global.spotfire.image.pullPolicy.
  pullPolicy:
  ## Optionally specify an array of imagePullSecrets.
  # image.pullSecrets -- The spotfire-server image pull secrets.
  pullSecrets: []

fluentBitSidecar:
  image:
    # -- The image repository for the fluent-bit logging sidecar.
    repository: fluent/fluent-bit
    # -- The image tag to use for the fluent-bit logging sidecar.
    tag: "1.9.8"
    # -- The image pull policy for the fluent-bit logging sidecar image.
    pullPolicy: IfNotPresent

  # -- The securityContext setting for the fluent-bit sidecar container. Overrides any securityContext setting on the Pod level.
  securityContext: {}
    # # -- Enable this if running as privileged.
    # privileged: false
    # # -- Enable this if running as NonRoot User.
    # runAsNonRoot: true
    # # -- User ID for the Container.
    # runAsUser: 1000
    # # -- Group ID for the Container.
    # runAsGroup: 0

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created.
  create: false
  # Annotations to add to the service account.
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, then a name is generated using the fullname template.
  name: ""

podAnnotations:
  prometheus.io/path: /spotfire/metrics
  prometheus.io/port: "9080"
  prometheus.io/scrape: "true"

# -- The Pod securityContext setting applies to all of the containers inside the Pod.
podSecurityContext: {}
  # # -- Owner User ID for the volume and any file created in that volume.
  # fsGroup: 1000
  # # -- User ID for the Pod.
  # runAsUser: 1000
  # # -- Group ID for the Pod.
  # runAsGroup: 0

# -- The securityContext setting for the spotfire-webplayer container. Overrides any securityContext setting on the Pod level.
securityContext: {}
  # # -- Enable this if running as privileged.
  # privileged: false
  # # -- Enable this if running as NonRoot User.
  # runAsNonRoot: true
  # # -- User ID for the Container.
  # runAsUser: 1000
  # # -- Group ID for the Container.
  # runAsGroup: 0

service:
  type: ClusterIP
  port: 9501

## The Spotfire node manager startup, readiness, and liveness probe initial delay and timeout.
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
startupProbe:
  enabled: true
  httpGet:
    path: /spotfire/started
    port: registration
  initialDelaySeconds: 60
  periodSeconds: 3
  failureThreshold: 20
livenessProbe:
  enabled: true
  httpGet:
    path: /spotfire/liveness
    port: registration
  initialDelaySeconds: 60
  periodSeconds: 3
  failureThreshold: 10
readinessProbe:
  enabled: false
  httpGet:
    path: /spotfire/readiness
    port: registration
  initialDelaySeconds: 60
  periodSeconds: 3
  failureThreshold: 10

logging:
  # -- This should be the spotfire-server log-forwarder name
  logForwarderAddress: ""
  ## -- set to `debug`, `trace`, `minimal` or leave empty for info
  logLevel: "debug"

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This increases chances that charts can run on environments with few 
  # resources, such as Minikube. If you do want to specify resources, then uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# -- KEDA autoscaling configuration. See https://keda.sh/docs/latest/concepts/scaling-deployment for more details.
# @default -- Disabled
kedaAutoscaling:
  enabled: false
  # -- The interval to check each trigger on.
  pollingInterval: 30
  # -- The period to wait after the last trigger reported active before scaling the resource back to 0.
  cooldownPeriod: 300
  # -- The minimum number of replicas KEDA scales the resource down to.
  minReplicas: 1
  # -- This setting is passed to the HPA definition that KEDA creates for a given resource and holds the maximum number of replicas of the target resource.
  maxReplicas: 4
  # threshold: 4
  # fallback:
  # advanced:
  # -- Spotfire specific settings
  spotfireConfig:
    # -- REQUIRED The URL to the Prometheus server where metrics should be fetched from.
    prometheusServerAddress: http://prometheus-server.monitor.svc.cluster.local

nodeSelector: {}

tolerations: []

affinity: {}

config:
  # -- A custom [Spotfire.Dxp.Worker.Core.config](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/spotfire.dxp.worker.core.config_file.html).
  Spotfire.Dxp.Worker.Core.config: ""
  # -- A custom Spotfire.Dxp.Worker.Host.dll.config. See [Spotfire.Dxp.Worker.Host.exe.config](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/spotfire.dxp.worker.host.exe.config_file.html).
  Spotfire.Dxp.Worker.Host.dll.config: ""
  # -- A custom [Spotfire.Dxp.Worker.Web.config](https://docs.tibco.com/pub/spotfire_server/latest/doc/html/TIB_sfire_server_tsas_admin_help/server/topics/spotfire.dxp.worker.web.config_file.html).
  Spotfire.Dxp.Worker.Web.config: ""
  # --
  # log4net.config:

# -- Additional environment variables.
extraEnvVars: []
#  - name: NAME
#    value: value

# The name of the ConfigMap containing additional environment variables.
extraEnvVarsCM: ""

# The name of the Secret containing extra additional environment variables
extraEnvVarsSecret: ""

# -- Extra volumeMounts for the spotfire-webplayer container.
# More info: `kubectl explain deployment.spec.template.spec.containers.volumeMounts`
extraVolumeMounts: []
  # - name: example
  #   mountPath: /opt/tibco/example.txt
  #   subPath: example.txt

# -- Extra volumes for the spotfire-webplayer container.
# More info: `kubectl explain deployment.spec.template.spec.volumes`
extraVolumes: []
  # - name: example
  #   persistentVolumeClaim:
  #     claimName: exampleClaim

# -- Additional init containers to add to the webplayer pod.
extraInitContainers: []
  # - name: dummy-init
  #   image: busybox
  #   command: ['echo', "hey"]

volumes:
  troubleshooting:
    persistentVolumeClaim:
       # -- If 'true', then a 'PersistentVolumeClaim' is created.
      create: false
      # -- Specifies the name of the 'StorageClass' to use for the volumes.troubleshooting-claim.
      storageClassName: ""
      # -- Specifies the standard Kubernetes resource requests and/or limits for the volumes.troubleshooting claims.
      resources:
        requests:
          storage: 2Gi
      # -- Specifies the name of the persistent volume to use for the volumes.troubleshooting-claim.
      volumeName:
    # -- When 'persistentVolumeClaim.create' is 'false', then use this value to define an already-existing
    # persistent volume claim
    existingClaim: ""